\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage[margin=0.5in]{geometry}
\usepackage[fleqn]{amsmath}
\usepackage{listings}

\DeclareMathOperator*{\argmin}{argmin}

\title{
    Machine Learning and Data Mining I$\colon$Lecture 2\\
    Linear Regression
}
\author{Morgan McCarty}
\date{05 July 2023}

\begin{document}
    \maketitle
    \section{Problem Setup}
        \begin{itemize}
            \item We are going to predict the market value of a house.
            \item Our Task: predict the value of a house given the number of rooms.
            \item The x-axis: will be the input feature values (in this case the number of rooms).
            \item The y-axis: will be the output/target values (in this case the price of the house).
            \item A domain expert says this model should be linear.
        \end{itemize}
    \section{The Model}
        \begin{itemize}
            \item We want to find a function that can map our input (a house and it's size) to our output (the house's price).
            \item To do this we use a linear model.
            \\For example:
            \begin{equation}
                y = w_0 + w_1x
            \end{equation}
            \item $w_0$ is the bias term (the y-intercept).
            \item $w_1$ is the weight of the feature (the slope). This feature is the number of rooms.
            \item We need to find the best values for $w_0$ and $w_1$ so that our model fits the data.
            \item Our dataset contains $N$ examples, as such we can write $N$ equations:
            \begin{align*}
                y_1 &= w_0 + w_1x_1\\
                y_2 &= w_0 + w_1x_2\\
                &\vdots\\
                y_N &= w_0 + w_1x_N
            \end{align*}
            \item This system of equations is overdetermined (we have more equations than unknowns). As such we cannot find a solution that satisfies all of the equations, but we can find a good approximation.
            \item How can we characterize what approximation is better than another?
            \item Assume we have a line $l_1$ and a line $l_2$ that both fit the data. Which line is better?
            \item Both lines will have an error value for each point. We can sum these errors to get a total error for each line.
            \item However we can't just sum the errors because some errors will be positive and some will be negative.
            \item There are two common ways to deal with this:
            \begin{itemize}
                \item Sum the absolute values of the errors (L1 norm)
                \item Sum the squares of the errors (L2 norm)
            \end{itemize}
            \item The L2 norm is more common because it is easier to work with.
        \end{itemize}
        \subsection{The Loss Function}
            \begin{itemize}
                \item We will write an equation which represents the $i$th observation in our dataset.
                \begin{equation}
                    \hat{y}_i = w_0 + w_1x_i
                \end{equation}
                \item The error for the $i$th observation is:
                \begin{equation}
                    e_i = {(\hat{y}_i - y_i)}^2 = {(w_0 + w_1x_i - y_i)}^2
                \end{equation}
                \item The total error is (the sum of the errors for each observation / the sum of the squared errors):
                \begin{equation}
                    E(w) = \sum_{i=1}^N e_i = \sum_{i=1}^N {(w_0 + w_1x_i - y_i)}^2
                \end{equation}
            \end{itemize}
            \subsubsection{Minimizing the Loss}
                \begin{itemize}
                    \item We are looking for the values of $w_0$ and $w_1$ that minimize the loss function:
                    \begin{equation}
                        \argmin_{w_0, w_1} E(w)
                    \end{equation}
                    \item We can find the minimum of a function by taking the derivative and setting it equal to zero.
                    \item Consider the example:
                    \begin{equation}
                        f(x) = 3x^2 + 4x + 1
                    \end{equation}
                    \item We want to find $x^*$ such that $f(x^*) \leq f(x)$ for all $x$.
                    \item In this case we can analytically find this value.
                    \begin{align*}
                        f'(x) &= 6x + 4\\
                        0 &= 6x + 4\\
                        x^* &= -\frac{2}{3}
                    \end{align*}
                    \item We can verify that this is a minimum by taking the second derivative and verifying that it is positive.
                    \begin{align*}
                        f''(x) &= 6\\
                        f''(x^*) &= 6 > 0
                    \end{align*}
                    \item Alternatively we can work iteratively to find the minimum.
                    \begin{itemize}
                        \item Start with a random value for $x$. Let's say $x_0 = 1$.
                        \item Find the derivative at $x_0$.
                        \begin{align*}
                            f'(x_0) &= 6(1) + 4 = 10
                        \end{align*}
                        \item Move a small amount in the direction of the derivative. Let's say we move $0.1$. (This value is called the learning rate or step size and is represented by $\eta$.)
                        \begin{align*}
                            x_1 &= x_0 - \eta f'(x_0)\\
                            x_1 &= 1 - 0.1(10)\\
                            x_1 &= 0
                        \end{align*}
                        \item Repeat this process until the derivative is close to zero.
                        \item This process is called gradient descent or the method of steepest ascent. (In this case the gradient is the derivative with respect to $x$ as there is only one variable.)
                        \item Here is an example with code:
                        \begin{lstlisting}[language=Python,gobble=28]
                            def f(x):
                                return 3 * x ** 2 + 4 * x + 1

                            def df(x):
                                return 6 * x + 4

                            def gradient_descent(x, eta, iterations):
                                for i in range(iterations):
                                    x = x - eta * df(x)
                                return x

                            x = gradient_descent(1, 0.1, 100)
                            print(x)
                        \end{lstlisting}
                        \item This code outputs $-0.6666666666666667$ which is close to the value we found analytically.
                        \item We can now write the gradient descent algorithm for our loss function:
                        \begin{lstlisting}[language=Python,gobble=28]
                            import numpy as np

                            def E(w, x, y): # loss function
                                return np.sum((w[0] + w[1] * x - y) ** 2)

                            def gradient_descent(w, x, y, eta, iterations):
                                for i in range(iterations):
                                    w = w - eta * np.array([
                                        np.sum(2 * (w[0] + w[1] * x - y)),
                                        np.sum(2 * (w[0] + w[1] * x - y) * x)
                                    ])
                                return w

                            x = np.array([1, 2, 3, 4, 5]) # number of rooms
                            y = np.array([1, 3, 2, 3, 5]) # price of house
                            w = gradient_descent(np.array([1, 1]), x, y, 0.1, 100)
                            print(w)
                        \end{lstlisting}
                        \item This code outputs $[0.2, 0.8]$.
                        \item In this case we can also find the minimum analytically.
                        \item We can write the loss function as a matrix equation:
                        \begin{align*}
                            E(w) &= \sum_{i=1}^N {(w_0 + w_1x_i - y_i)}^2\\
                            &= {(w_0 + w_1x_1 - y_1)}^2 + {(w_0 + w_1x_2 - y_2)}^2 + \cdots + {(w_0 + w_1x_N - y_N)}^2\\
                            &= \begin{bmatrix}
                                w_0 + w_1x_1 - y_1\\
                                w_0 + w_1x_2 - y_2\\
                                \vdots\\
                                w_0 + w_1x_N - y_N
                            \end{bmatrix}^T
                            \begin{bmatrix}
                                w_0 + w_1x_1 - y_1\\
                                w_0 + w_1x_2 - y_2\\
                                \vdots\\
                                w_0 + w_1x_N - y_N
                            \end{bmatrix}\\
                            &= {(Xw - y)}^T(Xw - y)
                        \end{align*}
                        \item Where $X$ is the design matrix (the matrix of input values, in this case the number of rooms. It is also called the feature matrix or the Regressor.) The first column is all ones to account for the bias term:
                        \begin{equation}
                            X = \begin{bmatrix}
                                1 & x_1\\
                                1 & x_2\\
                                \vdots & \vdots\\
                                1 & x_N
                            \end{bmatrix}
                        \end{equation}
                        \item For our loss function we have two variables ($w_0$ and $w_1$) so we need to take the partial derivative with respect to each variable.
                        \item We can calculate the partial derivative with respect to $w_0$:
                        \begin{align*}
                            \frac{\partial E(w)}{\partial w_0} &= \frac{\partial}{\partial w_0} \sum_{i=1}^N {(w_0 + w_1x_i - y_i)}^2\\
                            &= \sum_{i=1}^N \frac{\partial}{\partial w_0} {(w_0 + w_1x_i - y_i)}^2\\
                            &= \sum_{i=1}^N 2(w_0 + w_1x_i - y_i)\\
                            &= 2 \sum_{i=1}^N (w_0 + w_1x_i - y_i)
                        \end{align*}
                        \item We can calculate the partial derivative with respect to $w_1$:
                        \begin{align*}
                            \frac{\partial E(w)}{\partial w_1} &= \frac{\partial}{\partial w_1} \sum_{i=1}^N {(w_0 + w_1x_i - y_i)}^2\\
                            &= \sum_{i=1}^N \frac{\partial}{\partial w_1} {(w_0 + w_1x_i - y_i)}^2\\
                            &= \sum_{i=1}^N 2(w_0 + w_1x_i - y_i)x_i\\
                            &= 2 \sum_{i=1}^N (w_0 + w_1x_i - y_i)x_i
                        \end{align*}
                        \item We can now write the gradient of the loss function:
                        \begin{align*}
                            \nabla E(w) &= \begin{bmatrix}
                                \frac{\partial E(w)}{\partial w_0}\\
                                \frac{\partial E(w)}{\partial w_1}
                            \end{bmatrix}\\
                            &= \begin{bmatrix}
                                2 \sum_{i=1}^N (w_0 + w_1x_i - y_i)\\
                                2 \sum_{i=1}^N (w_0 + w_1x_i - y_i)x_i
                            \end{bmatrix}
                        \end{align*}
                        \item Now to find the minimum we can set the gradient equal to zero and solve for $w$:
                        \begin{align*}
                            \nabla E(w) &= \begin{bmatrix}
                                2 \sum_{i=1}^N (w_0 + w_1x_i - y_i)\\
                                2 \sum_{i=1}^N (w_0 + w_1x_i - y_i)x_i
                            \end{bmatrix}\\
                            &= \begin{bmatrix}
                                0\\
                                0
                            \end{bmatrix}\\
                            \begin{bmatrix}
                                2 \sum_{i=1}^N (w_0 + w_1x_i - y_i)\\
                                2 \sum_{i=1}^N (w_0 + w_1x_i - y_i)x_i
                            \end{bmatrix} &= \begin{bmatrix}
                                0\\
                                0
                            \end{bmatrix}\\
                            \begin{bmatrix}
                                \sum_{i=1}^N (w_0 + w_1x_i - y_i)\\
                                \sum_{i=1}^N (w_0 + w_1x_i - y_i)x_i
                            \end{bmatrix} &= \begin{bmatrix}
                                0\\
                                0
                            \end{bmatrix}\\
                            \begin{bmatrix}
                                \sum_{i=1}^N w_0 + \sum_{i=1}^N w_1x_i - \sum_{i=1}^N y_i\\
                                \sum_{i=1}^N w_0x_i + \sum_{i=1}^N w_1x_i^2 - \sum_{i=1}^N {x_i}y_i
                            \end{bmatrix} &= \begin{bmatrix}
                                0\\
                                0
                            \end{bmatrix}\\
                            \begin{bmatrix}
                                Nw_0 + w_1\sum_{i=1}^N x_i - \sum_{i=1}^N y_i\\
                                w_0\sum_{i=1}^N x_i + w_1\sum_{i=1}^N x_i^2 - \sum_{i=1}^N {x_i}y_i
                            \end{bmatrix} &= \begin{bmatrix}
                                0\\
                                0
                            \end{bmatrix}\\
                            \begin{bmatrix}
                                Nw_0 + w_1\sum_{i=1}^N x_i\\
                                w_0\sum_{i=1}^N x_i + w_1\sum_{i=1}^N x_i^2
                            \end{bmatrix} &= \begin{bmatrix}
                                \sum_{i=1}^N y_i\\
                                \sum_{i=1}^N {x_i}y_i
                            \end{bmatrix}\\
                            \begin{bmatrix}
                                N & \sum_{i=1}^N x_i\\
                                \sum_{i=1}^N x_i & \sum_{i=1}^N x_i^2
                            \end{bmatrix} \begin{bmatrix}
                                w_0\\
                                w_1
                            \end{bmatrix} &= \begin{bmatrix}
                                \sum_{i=1}^N y_i\\
                                \sum_{i=1}^N {x_i}y_i
                            \end{bmatrix}\\
                            \begin{bmatrix}
                                w_0\\
                                w_1
                            \end{bmatrix} &= \begin{bmatrix}
                                N & \sum_{i=1}^N x_i\\
                                \sum_{i=1}^N x_i & \sum_{i=1}^N x_i^2
                            \end{bmatrix}^{-1} \begin{bmatrix}
                                \sum_{i=1}^N y_i\\
                                \sum_{i=1}^N {x_i}y_i
                            \end{bmatrix}
                        \end{align*}
                        \item These equations are called the normal equations.
                        \item Not all matrices are invertible, and inverting a matrix is computationally expensive. So not all problems can be solved this way.
                        \item With this loss function we can generalize to more than one feature.
                        \item For example, if we have two features (number of rooms and square footage) we can write the loss function as:
                        \begin{equation}
                            E(w) = \sum_{i=1}^N {(w_0 + w_1x_{i1} + w_2x_{i2} - y_i)}^2
                        \end{equation}
                        \item The normal equations become:
                        \begin{align*}
                            \begin{bmatrix}
                                w_0\\
                                w_1\\
                                w_2
                            \end{bmatrix} &= \begin{bmatrix}
                                N & \sum_{i=1}^N x_{i1} & \sum_{i=1}^N x_{i2}\\
                                \sum_{i=1}^N x_{i1} & \sum_{i=1}^N x_{i1}^2 & \sum_{i=1}^N x_{i1}x_{i2}\\
                                \sum_{i=1}^N x_{i2} & \sum_{i=1}^N x_{i1}x_{i2} & \sum_{i=1}^N x_{i2}^2
                            \end{bmatrix}^{-1} \begin{bmatrix}
                                \sum_{i=1}^N y_i\\
                                \sum_{i=1}^N {x_{i1}}y_i\\
                                \sum_{i=1}^N {x_{i2}}y_i
                            \end{bmatrix}
                        \end{align*}
                        \item We can therefore write the normal equations in general as:
                        \begin{equation}
                            w = {({X^T}X)}^{-1}{X^T}y
                        \end{equation}
                        \item Where $X$ is the design matrix and $y$ is the vector of target values.
                        \item This is called the closed form solution.
                        \item A note: often $\frac{1}{2}$ is included in the loss function. This is done to simplify the derivative. It does not change the minimum.
                    \end{itemize}
                    \subsubsection{Why Use a Dummy/Bias Term?}
                        \begin{itemize}
                            \item The dummy term will always be multiplied by one.
                            \item It is essentially a constant that can be added to the model to shift it up or down.
                            \item Without the bias term the model will always go through the origin.
                            \item This is problematic because the origin is not always in the data.
                        \end{itemize}
                    \subsubsection{Gradient Descent Explained}
                        \begin{itemize}
                            \item Gradient descent is an iterative method for finding the minimum of a function.
                            \item Assume we have a Loss Function $J(w)$ (e.g.\ our Sum of Squared Errors).
                            \item Let's say this function is:
                            \begin{equation}
                                J(w) = \frac{1}{2} \sigma_{i=1}^N {({w^T}x_i - y_i)}^2
                            \end{equation}
                            \item We want to find the value of $w$ (which is a vector) that minimizes this function. Say $w^*$.
                            \item So this is:
                            \begin{equation}
                                w^* = \argmin_w J(w)
                            \end{equation}
                            \item We now need to find the gradient of $J(w)$.
                            \item As such we take the partial derivative of $J(w)$ with respect to each element of $w$.
                            \item We can write this as:
                            \begin{equation}
                                \frac{\partial}{\partial w_k} J = \frac{1}{2} \frac{\partial}{\partial w_k} \Sigma_{i=1}^N {({w^T}x_i - y_i)}^2
                            \end{equation}
                            \item We can now use the chain rule to expand this equation:
                            \begin{equation}
                                \frac{\partial}{\partial w_k} J = \frac{1}{2} \Sigma_{i=1}^N 2({w^T}x_i - y_i) \frac{\partial}{\partial w_k} ({w^T}x_i - y_i)
                            \end{equation}
                            \item We can now take the partial derivative of $({w^T}x_i - y_i)$ with respect to $w_k$:
                            \begin{equation}
                                \frac{\partial}{\partial w_k} ({w^T}x_i - y_i) = x_{ik}
                            \end{equation}
                            \item We can now substitute this back into our equation:
                            \begin{equation}
                                \frac{\partial}{\partial w_k} J = \frac{1}{2} \Sigma_{i=1}^N 2({w^T}x_i - y_i) x_{ik}
                            \end{equation}
                            \item We can now simplify this equation:
                            \begin{equation}
                                \frac{\partial}{\partial w_k} J = \Sigma_{i=1}^N ({w^T}x_i - y_i) x_{ik}
                            \end{equation}
                            \item Now we have our partial derivative with respect to $w_k$.
                            \item Next we need to create our update rule.
                            \item This is an equation that will update the value of $w$ (and hence all of the weights in our model) along the gradient.
                            \item We can write a version which updates one weight at a time:
                            \begin{equation}
                                w_{k+1}^{i+1} = w_k^i - \eta \Sigma_{i=1}^N ({w^T}x_i - y_i) x_{ik}
                            \end{equation}
                            \item Where $\eta$ is the learning rate.
                            \item Or a version which updates all of the weights at once:
                            \begin{equation}
                                w^{i+1} = w^i - \eta \Sigma_{i=1}^N ({w^T}x_i - y_i) x_i
                            \end{equation}
                            \item In these cases $x_i$ is the $i$th row of the design matrix, $x_{ik}$ is the $k$th element of the $i$th row of the design matrix, and $y_i$ is the $i$th element of the target vector.
                            \item Now we have our update rule and we can start with a random value for $w$ and update it until it converges or we reach a maximum number of iterations.
                        \end{itemize}
                \end{itemize}

\end{document}